<!DOCTYPE html>
<!-- saved from url=(0028)https://wencheng-zhu.github.io/ -->
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>朱文成(Wencheng Zhu)</title>
  
  <meta name="author" content="Wencheng Zhu">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/jpg" href="wencheng zhu_files/pic3.png">
</head>

<body data-new-gr-c-s-check-loaded="14.984.0" data-gr-ext-installed="">
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name></name>
              </p>
              <p> 朱文成，天津大学人工智能学院副研究员，水木学者，硕士生导师，55教学楼B519。
                2014和2017年取得天津大学学士和硕士学位，指导老师是胡清华教授和朱鹏飞教授；
                2021年6月取得清华大学博士学位，导师是鲁继文长聘教授；
                2021年7月清华大学水利系做博士后研究，合作导师是张建民院士。
                2023年11月入职天津大学人工智能学院，
                主要关注视觉-语言大模型的感知、推理与强化学习策略。招收有自驱力和好奇心的博士生（联合指导）和硕士生，<b>一起点亮未知，让科研成为一场充满惊喜的旅程！</b>
              </p>
              <p>
                I obtained my Bachelor and Master degrees from Tianjin University of Computer Science and Technology, advised by Prof. <a href="https://scholar.google.com/citations?user=TVSNq_wAAAAJ&hl=zh-CN&oi=ao/">Qinghua Hu</a> and <a href="http://cic.tju.edu.cn/faculty/zhupengfei/index.html">Pengfei Zhu</a>.
                I received my Doctoral degree at the Department of Automation, Tsinghua University, in Intelligent Vision Group, under the supervision of Prof. <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>.
                I conducted postdoctoral research at Civil and Hydraulic Engineering， Tsinghua University, under the supervision of Prof. <a href="https://www.tsinghua.edu.cn/info/1166/93899.htm">Jian-Min Zhang</a>, an academician of the Chinese Academy of Engineering.
              </p>
              <p>
                I joined the School of Artificial Intelligence at Tianjin University in November 2023, focusing on 
                perception, reasoning, and reinforcement learning strategies of vision-language large models.
              </p>
              <p style="text-align:center">
                <a href="https://scholar.google.com/citations?user=ttXV3pIAAAAJ&hl=zh-CN"> Google Scholar </a> &nbsp;/&nbsp;
                <a href="https://github.com/wencheng-zhu"> GitHub </a> &nbsp;/&nbsp;
    <a href="mailto:zhu1992719@foxmail.com"> Email </a> 
              </p>
              <p style="text-align:center; color:red; font-weight:bold;">
                Drop me an email （wenchengzhu AT tju dot edu dot cn） for the 2025 and 2026 master and phd application !!!
              </p>
                
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%">
              <a href="https://wencheng-zhu.github.io/wencheng zhu_files/pic3.png"><img style="width:80%;max-width:80%" alt="profile photo" src="./wencheng zhu_files/pic3.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
    
    
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News</heading>
            <p>
              </p>
            <li style="margin: 5px;">
                <b>2025-05:</b> One paper on knowledge distillation is accepted to <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=11021326"> IEEE Transactions on Image Processing 2025</a>.
              </li>
            <li style="margin: 5px;">
                <b>2025-03:</b> One paper on video recognition is submitted to <a href="https://arxiv.org/pdf/2503.18407.pdf">Arxiv</a>.
              </li>
            <li style="margin: 5px;">
                <b>2024-08:</b> The National Natural Science Foundation Youth Fund was funded.
              </li>
             <li style="margin: 5px;">
                <b>2024-05:</b> One paper on knowledge distillation is submitted to <a href="https://arxiv.org/pdf/2404.14109.pdf">Arxiv</a>.
              </li>
             <li style="margin: 5px;">
                <b>2022-03:</b> One paper on video summarization is accepted to <a href="https://ieeexplore.ieee.org/document/9750933"> IEEE Transactions on Image Processing 2022</a>.
              </li>
            <li style="margin: 5px;">
                <b>2021-09:</b> One paper on video summarization is accepted to <a href="https://www.sciencedirect.com/science/article/pii/S0031320321004921"> Pattern Recognition 2021</a>.
              </li>
            <li style="margin: 5px;">
                <b>2021-06:</b> Selected into the Shuimu Tsinghua Scholar Program.
              </li>
             <li style="margin: 5px;">
                <b>2021-02:</b> One paper on video object segmentation is accepted to <a href="https://ieeexplore.ieee.org/document/9356697/">IEEE Transactions on Circuits and Systems for Video &nbsp &nbsp&nbsp&nbsp &nbsp &nbspTechnology 2021</a>.
              </li>
              <li style="margin: 5px;">
                <b>2020-11:</b> One paper on video summarization is accepted to <a href="https://ieeexplore.ieee.org/document/9275314/">IEEE Transactions on Image Processing 2020</a>.
              </li>
              <li style="margin: 5px;">
                <b>2019-09:</b> One paper on subspace clustering is accepted to <a href="https://www.sciencedirect.com/science/article/pii/S0031320319301797/">Pattern Recognition 2019</a>.
              </li>
              <li style="margin: 5px;">
                <b>2017-01:</b> One paper on subspace clustering is accepted to  <a href="https://www.sciencedirect.com/science/article/pii/S0031320319301797/"> Pattern Recognition 2017</a> 
              </li>
              <li style="margin: 5px;">
                <b>2016-08:</b> One paper on visual tracking is nominated for the best paper <a href="https://www.pricai.org/conferences/past-confs/17-pricai-2016-conference/">PRICAI 2016</a>.
              </li>
            <p></p>
          </td>
        </tr>
      </tbody></table>
    
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Selected Publications</heading>
        <p></p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="./wencheng zhu_files/vtd.jpg" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>VTD-CLIP: Video-to-Text Discretization via Prompting CLIP</papertitle>
              <br>
        <strong>Wencheng Zhu</strong>, Yuexin Wang, Hongxuan Li, Pengfei Zhu, and Qinghua Hu
              <br>
              <em>Arxiv </em>, 2025
        <br>
        <!-- <font color="red"><strong>Oral Presentation</strong></font> -->
        <br>
               <a href="https://arxiv.org/abs/2503.18407.pdf">[PDF]</a>
               <a href="https://github.com/isxinxin/VTD-CLIP">[Code]</a>
              <a href="https://wencheng-zhu.github.io/wencheng zhu_files/CKD.bib">[bibtex]</a>
              <br>
              <p> Our method repurposes the frozen text encoder to construct a visual codebook from video class labels due to the many-to-one contrastive alignment between visual and textual embeddings in multimodal pretraining. </p>
            </td>
      </tr>
          
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="./wencheng zhu_files/ckd-overview.jpg" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>CKD: Contrastive Knowledge Distillation from A Sample-wise Perspective</papertitle>
              <br>
        <strong>Wencheng Zhu</strong>, Xin Zhou, Pengfei Zhu, Yu Wang, and Qinghua Hu
              <br>
              <em>IEEE Transactions on Image Processing </em>, 2025
        <br>
        <br>
               <a href="https://arxiv.org/pdf/2404.14109.pdf">[PDF]</a>
               <a href="https://github.com/ZhouXinzzzzz/CKD">[Code]</a>
              <a href="https://wencheng-zhu.github.io/wencheng zhu_files/CKD.bib">[bibtex]</a>
              <br>
              <p> We propose a sample-wise contrastive knowledge distillation approach. </p>
            </td>
      </tr>
          
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="./wencheng zhu_files/RR_STG.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Relational Reasoning over Spatial-Temporal Graphs for Video Summarization</papertitle>
              <br>
                <strong>Wencheng Zhu</strong>, Yucheng Han, Jiwen Lu, and Jie Zhou
              <br>
              <em>IEEE Transactions on Image Processing (TIP)</em>, 2022
            <br>
            <br>
              <a href="https://ieeexplore.ieee.org/document/9750933">[PDF]</a>
              <a href="https://wencheng-zhu.github.io/wencheng zhu_files/RR_STG.bib">[bibtex]</a>
              <br>
              <p> We conduct relational reasoning over spatial-temporal graphs for video summarization. </p>
            </td>
       </tr>
       
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="./wencheng zhu_files/mha-overview.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Learning Multiscale Hierarchical Attention for Video Summarization</papertitle>
              <br>
        <strong>Wencheng Zhu</strong>, Jiwen Lu, Yucheng Han, and Jie Zhou
              <br>
              <em>Pattern Recognition (PR)</em>, 2021
        <br>
        <!-- <font color="red"><strong>Oral Presentation</strong></font> -->
        <br>
               <a href="https://www.sciencedirect.com/science/article/pii/S0031320321004921">[PDF]</a>
              <a href="https://wencheng-zhu.github.io/wencheng zhu_files/MHVS.bib">[bibtex]</a>
              <br>
              <p> We propose a multiscale hierarchical attention approach for supervised video summarization. </p>
            </td>
      </tr>
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="./wencheng zhu_files/ssm_vos.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Separable Structure Modeling for Semi-supervised Video Object Segmentation</papertitle>
              <br>
        <strong>Wencheng Zhu</strong>, Jiahao Li, Jiwen Lu, and Jie Zhou
              <br>
              <em>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)</em>, 2021
        <br>
        <!-- <font color="red"><strong>Oral Presentation</strong></font> -->
              <br>
<!--               <a href="https://ieeexplore.ieee.org/document/9356697">[PDF]</a>
               <a href="https://github.com/li-plus/DSNet">[Code]</a> 
              <a href="https://wencheng-zhu.github.io/wencheng zhu_files/DSNet.bib">[bibtex]</a> -->
              <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9356697">[PDF]</a>
              <a href="https://github.com/li-plus/SSM-VOS">[Code]</a> 
              <a href="https://www.youtube.com/watch?v=ufbjU86dhVc">[Video]</a>
              <br>
              <p> We propose a separable structure modeling approach for semi-supervised video object segmentation. </p>
            </td>
      </tr>
        <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="./wencheng zhu_files/dsnet.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>DSNet: A Flexible Detect-to-Summarize Network for Video Summarization</papertitle>
              <br>
        <strong>Wencheng Zhu</strong>, Jiwen Lu, Jiahao Li, and Jie Zhou
              <br>
              <em>IEEE Transactions on Image Processing (TIP)</em>, 2020
        <br>
        <!-- <font color="red"><strong>Oral Presentation</strong></font> -->
              <br>
              <a href="https://ieeexplore.ieee.org/document/9275314">[PDF]</a>
               <a href="https://github.com/li-plus/DSNet">[Code]</a> 
              <a href="https://wencheng-zhu.github.io/wencheng zhu_files/DSNet.bib">[bibtex]</a>
              <br>
              <p> We propose a Detect-to-Summarize network for video summarization. </p>
            </td>
      </tr>
          
      <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="./wencheng zhu_files/SMSC.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Structured General and Specific Multi-view Subspace Clustering</papertitle>
              <br>
        <strong>Wencheng Zhu</strong>, Jiwen Lu, and Jie Zhou
              <br>
              <em>Pattern Recognition (PR)</em>, 2019
        <br>
              <a href="https://www.sciencedirect.com/science/article/pii/S0031320319301797">[PDF]</a> 
        <!-- <a href="https://www.youtube.com/watch?v=zCTPRxxlZsI&amp;t=427s">[Video]</a>  -->
              <a href="https://wencheng-zhu.github.io/wencheng zhu_files/SiMSC.bib">[bibtex]</a>
              <br>
              <p> A multiple subspace clustering approach.</p>
            </td>
          </tr>
      
        <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="./wencheng zhu_files/NSC.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Nonlinear Subspace Clustering for Image Clustering</papertitle>
              <br>
        <strong>Wencheng Zhu</strong>, Jiwen Lu, and Jie Zhou
              <br>
              <em>Pattern Recognition Letters (PRL)</em>, 2018
        <br>
        <!-- <font color="red"><strong>Oral Presentation</strong></font> -->
              <br>
              <a href="https://wencheng-zhu.github.io/wencheng zhu_files/NSC.pdf">[PDF]</a>  
        <!-- <a href="https://www.youtube.com/watch?v=QkMiKE30Lv4">[Video]</a>  -->
        <a href="https://wencheng-zhu.github.io/wencheng zhu_files/NSC.bib">[bibtex]</a> 
              <br>
              <p> We propose a nonlinear subspace clustering approach for image clustering. </p>
            </td>
          </tr>
      
        <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="./wencheng zhu_files/NSC_Conf.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Nonlinear Subspace Clustering</papertitle>
              <br>
        <strong>Wencheng Zhu</strong>, Jiwen Lu, and Jie Zhou
              <br>
              <em>IEEE International Conference on Image Processing (ICIP)</em>, 2017
              <br>
              <a href="https://wencheng-zhu.github.io/wencheng zhu_files/NSC_Conf.pdf">[PDF]</a>  
        <!-- <a href="https://wencheng-zhu.github.io/wencheng zhu_files/NSC_Conf.bib">[PDF]</a>   -->
        <a href="https://wencheng-zhu.github.io/wencheng zhu_files/NSC_Conf.bib">[bibtex]</a> 
              <br>
              <p> We propose a nonlinear subspace clustering approach. </p>
            </td>
          </tr>
        <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="./wencheng zhu_files/SCUFS.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Subspace Clustering guided Unsupervised Feature Selection</papertitle>
              <br>
              Pengfei Zhu, <strong>Wencheng Zhu</strong>, Qinghua Hu, Changqing Zhang, Wangmeng Zuo
              <br>
              <em>Pattern Recognition (PR)</em>, 2017
              <br>
              <a href="https://wencheng-zhu.github.io/wencheng zhu_files/Subspace%20clustering%20guided%20unsupervised%20feature%20selection.pdf">[PDF]</a>  
        <a href="https://github.com/guangmingboy/Codes-for-SCUFS-FRFS0-SSVT">[Code]</a>  
        <a href="https://wencheng-zhu.github.io/wencheng zhu_files/SCUFS.bib">[bibtex]</a> 
              <br>
              <p> We propose a feature selection approach based on subspace clustering. </p>
            </td>
          </tr>
        <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="./wencheng zhu_files/nrss.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Non-convex Regularized Self-representation for Unsupervised Feature Selection</papertitle>
              <br>
              Pengfei Zhu,  <strong>Wencheng Zhu</strong>, Weizhi Wang, Wangmeng Zuo, Qinghua Hu
              <br>
              <em>Image and Vision Computing (IVC)</em>, 2016
              <br>
              <a href="https://www.sciencedirect.com/science/article/pii/S0262885616302049">[PDF]</a>  
        <a href="https://github.com/guangmingboy/Codes-for-SCUFS-FRFS0-SSVT">[Code]</a>  
        <a href="https://wencheng-zhu.github.io/wencheng zhu_files/NCRSR.bib">[bibtex]</a> 
              <br>
              <p> We propose a non-convex regularized feature selection approach. </p>
            </td>
          </tr>
        <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="./wencheng zhu_files/ssvt.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Set to Set Visual Tracking</papertitle>
              <br>
              <strong>Wencheng Zhu</strong>, Pengfei Zhu, Qinghua Hu, Changqing Zhang
              <br>
              <em>Pacific Rim International Conference on Artificial Intelligence (PRICAI)</em>, 2016
              <br>
              <a href="https://link.springer.com/chapter/10.1007/978-3-319-42911-3_59">[PDF]</a>  
              <a href="https://github.com/guangmingboy/Codes-for-SCUFS-FRFS0-SSVT">[Code]</a>  
              <a href="https://wencheng-zhu.github.io/wencheng zhu_files/SSVT.bib">[bibtex]</a> 
              <br>
              <p> We propose a visual tracking approach by using set to set distance.</p>
            </td>
          </tr>

      

        </tbody></table>
    
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Honors and Awards</heading>
            <p>
        </p>
            <li style="margin: 5px;">
               The Shuimu Tsinghua Scholar Program, Tsinghua University, 2021.
              </li>
            <li style="margin: 5px;">
                Outstanding Graduate, Tianjin University, 2017.
              </li>
            
            <li style="margin: 5px;">
                National Scholarship, Tianjin University, 2016-2017.
              </li>
<!--        <li style="margin: 5px;">
                Tianjin University, 2017.
              </li> -->
        <li style="margin: 5px;">
                Best Paper Nomination on PRICAI 2016.
              </li>
          <li style="margin: 5px;">
                Outstanding Graduate, Tianjin University, 2014.
              </li>
            <p></p>
          </td>
        </tr>
      </tbody></table>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Fundings and Projects</heading>
            <p>
        </p>
            <li style="margin: 5px;">
               The National Natural Science Foundation Youth Fund, 2025-2027.
              </li>
           
      </tbody></table>
        
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Professional Activities</heading>
            <p>
         </p>
            <li style="margin: 5px;">
                <b>Young editorial board of </b> Intelligent Security.
              </li>
            <li style="margin: 5px;">
                <b>Journal Reviewer of </b> Advanced Science, TIP, TNNLS, TKDE, TCYB, TMM, TCSVT, TOMM, TCSS, PR, PRL.
              </li>
              <li style="margin: 5px;">
                <b>Conference Reviewer of </b> CVPR, ICCV, ICLR, ICME, WACV, FG, ICIP, ICPR, VCIP, IScIDE.
              </li>
        <!-- <li style="margin: 5px;">
                <b>Reviewer, </b> IEEE Transactions on Circuits and Systems for Video Technology, 2019-.
              </li>
        <li style="margin: 5px;">
                <b>Reviewer, </b> Pattern Recognition, 2019-.
              </li>
              <li style="margin: 5px;">
                <b>Reviewer, </b> Pattern Recognition Letters, 2019-.
              </li>
        <li style="margin: 5px;">
                <b>Reviewer, </b> Journal of Visual Communication and Image Representation, 2018-.
              </li>
              <li style="margin: 5px;">
                <b>Reviewer,</b> IEEE International Conference on Multimedia and Expo, 2019-2020.
              </li>
              <li style="margin: 5px;">
                <b>Reviewer,</b> IEEE International Conference on Image Processing, 2018-2020.
              </li>
        <li style="margin: 5px;">
                <b>Reviewer,</b> International Conference on Pattern Recognition, 2018-2020.
              </li> -->
            <p></p>
          </td>
        </tr>
      </tbody></table>
        
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Students</heading>
            <p>
        </p>
            <li style="margin: 5px;">
               Hongxuan Li, Master candidate, 2025-2028.
            </li>
            <li style="margin: 5px;">
               Jiazhen Ma, Master candidate, 2025-2028.
            </li>
            <li style="margin: 5px;">
               Xiaokun Wang, Master candidate, 2025-2028.
            </li>
            <li style="margin: 5px;">
               Wenchang Xu, Master candidate, 2025-2028.
            </li>          
      </tbody></table>

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://jonbarron.info/">Website Template</a>
              </p>
            </td>
          </tr>
        </tbody></table>
       
      </td>
    </tr>
  </tbody></table>



</body></html>
